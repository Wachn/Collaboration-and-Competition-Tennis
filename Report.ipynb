{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "--------------------\n",
    "\n",
    "This Report notebook is a walkthrough in understanding the following in greater detail: *Learning Algorithm*, *Plot of Rewards*, *Ideas for Future Work*. This report will describe the learning algorithm and the chosen hyperparameters for the model architectures and the model itself. Lastly, it will discuss the plot of rewards to illustrate the agent's performance received reward (over 100 consecutive episodes). The given challenge will be to solve the environment by achieving average +0.5 rewards. \n",
    "\n",
    "\n",
    "Content\n",
    "---------------\n",
    "1. Learning Algorithm\n",
    "    - 1.1 DDPG agent\n",
    "    - 1.2 Model Architecture\n",
    "    - 1.3 Learning Parameters\n",
    "    - 1.4 Algorithm to train DDPG\n",
    "2. Plot of Rewards\n",
    "3. Ideas for Future Work\n",
    "    - 3.1 Model Architecture\n",
    "    - 3.2 Modify the Number of Agents\n",
    "    - 3.3 Modify the Learning Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Learning Algorithm\n",
    "-------------------------------\n",
    "Firstly this report will be discussing the agent before proceeding to discuss the model and algorithm.\n",
    "\n",
    "### 1.1 MADDPG\n",
    "\n",
    "In this section, we will understand this Multiagent Deep Deterministic Policy Gradient (MADDPG) agent. The main key feature of this algorithm is the adoption of additional agent. The critic net, with very similar functionality as the DDPG however takes in additional actions from the other agent, however each agent does not share similar observation (states) as they have their unique observation. The idea of MADDPG very similar to DDPG except we have to update the differnt agents separately and the agents share a replay buffer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Model Architecture\n",
    "\n",
    "The model adopted in general is based on linear layers. There are three for both actor and critic network. However, in the second FC layer of the critic network, it takes in the concatenation of the action matrix. Thus the network is not a straightforward FC layers for critic network unlike the actor network. In the final layer of the actor network, the output is the action dimensions while the output of the critic network is the state value. In both cases, both are initialise using the same seed for the same random weights.\n",
    "\n",
    "Usage of swish function is chosen here. Swish activation function is a smooth and non-monotonic function that is simpliy $swish(x) = x\\sigma(x)$. The idea of `ReLU` function has a draw back that approximately half of the input $x$ will result in a gradient that is 0. Previous LeakyReLU and SELU are unable to overcome this issue. Since we have design the action to be clipped between -1 and 1 it will make sense to incoporate negative input. Hence, swish function is used in the model instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Learning Parameters\n",
    "\n",
    "Hyperparameters play a crucial role in optimizing the network. With a set of good hyperparameters, the model can be optimized easily or the convergence for maximization of reward can be achieved quickly. In this work, there are a number of variables for optimizing, however these values are chosen as default in the generation of reward function to be shown in the *Plot of Rewards* section later. In MADDPG, both agents have similar network.\n",
    "_*List of Hyperparameters*_\n",
    "1. Seed of model: 10                                # Initialization value for the weights\n",
    "2. Hidden layer size: (400,300)                     # This could potentially directly affect the type of representations learned\n",
    "3. Batch size: 256                                  # The amount of experience to sample in on learning update\n",
    "4. Buffer size: 1e5                                # Storage size of the latest experiences\n",
    "5. Gamma: 0.995                                     # To modify the contribution to the target q-value\n",
    "6. Tau: 5e-3                                       # To modify the soft update of the target network\n",
    "7. Learning rate for actor: 1e-4                   # Tune the backpropagation sensitivity and impact\n",
    "8. Learning rate for critic: 5e-3\n",
    "8. Maximum timesteps per episode: 1000             # The amount of experience in one episode\n",
    "9. Decay Noise: (0.5, 0.999)                      # The (decay noise of the OU-noise input to action, decaying rate increment per timestep of the decay noise)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Algorithm to train MADDPG\n",
    "In considering the deep model that we have discussed, we will need an algorithm that runs recursively to (1) observe the current environment state, for a range of time steps the (2) agent acts on the given states, (3) receive feedback by receiving new states and rewards after acting on the environment, (4) agent updates itself, (5) check for the condition if the environment is solved. These steps are more or less a general approach to tackle MADDPG problems.\n",
    "\n",
    "Overall, the main bottleneck in optimising this algorithm to achieve ~0.5 avg scores liesMA in (i) Learning rate of the model, (ii) Model Architecture, (iii) Balancing of the exploration and exploitation process - OUNOISE optimising. While other parameters do affect the learning, more importantly these three are the ones the can affect the mean scores more effectively.\n",
    "\n",
    "In this training, the difficulty lies with determining the correct decay of the OUNoise which is essential in contributing to a better learned system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Plot of Rewards\n",
    "------------------------\n",
    "![Image](https://github.com/Wachn/Collaboration-and-Competition-Tennis/blob/newmodel/plots/MADDPG-Tennis.png?raw=true)\n",
    "In the plot shown above, the average rewards varies greatly towards 1000 episodes. The Figure in the left shows a spike high towards 2.5 score while the averaged over 100 episodes reaches our condition to solve the environment where average score of +0.5(0ver 100 consecutive episodes, after the maximum over both agents). The two figures below depicts the scores of Agent-0 (Left) and Agent-1(Right), as we can see both experiences different scores, however both share the same narrative of exploding spur of scores towards 1000 episodes.            \n",
    "<img style=\"float: left; width:400px;height:400px;\" src=\"https://raw.githubusercontent.com/Wachn/Collaboration-and-Competition-Tennis/217950bc100be610a53294684dd761e6b44e32a4/plots/Agent0_mean_episode_rewards.svg\">\n",
    "<img style=\"float: left; width:400px;height:400px;\" src=\"https://raw.githubusercontent.com/Wachn/Collaboration-and-Competition-Tennis/217950bc100be610a53294684dd761e6b44e32a4/plots/Agent1_mean_episode_rewards.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Ideas for Future Work\n",
    "------------------------\n",
    "### 3.1 Model Architecture:\n",
    "While there are no graphs, in this work it is found that by tunning the model, we can achieve varying performance. For instance in the deep model used, by tuning the feature dimension of the FC layer we can achieve varying results. Understandably, we can find a better model to optimise the entire network.\n",
    "\n",
    "### 3.2 Decay Learning Rate\n",
    "Current the learning scheduler was switched off. Further convergence for optimization can be conducted by reducing the learning rate of actor and critic networks.\n",
    "\n",
    "### 3.3 Modify the Batch Size\n",
    "Altering the batch size can affect the learning procedures.\n",
    "\n",
    "### 3.4 Prioritised Experience Replay\n",
    "In such learning environment prioritised experience replay could be helpful as we eliminate harmful instances that might cause the learning to become unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
